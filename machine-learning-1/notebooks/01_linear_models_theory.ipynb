{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c621e5-3e22-493e-9119-7cd9c2031ac2",
   "metadata": {},
   "source": [
    "## Purpose of this Notebook\n",
    "\n",
    "This notebook contains theoretical foundations of linear models for Machine Learning 1.\n",
    "The focus is on mathematical understanding, statistical inference, and structured derivations.\n",
    "Implementation examples are provided in separate notebooks.\n",
    "\n",
    "# Machine Learning 1 — Linear Models\n",
    "\n",
    "**Author:** Elena Fuchs  \n",
    "**Course:** ML1  \n",
    "**Topic:** Linear Regression Foundations  \n",
    "\n",
    "---\n",
    "\n",
    "# What is a Linear Model?\n",
    "\n",
    "A linear model assumes that the relationship between a response variable $Y$  \n",
    "and one or more predictors $X$ can be described as:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\beta_0$ = intercept  \n",
    "- $\\beta_1$ = slope  \n",
    "- $\\varepsilon$ = error term  \n",
    "\n",
    "More generally, for $n$ observations:\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Multiple Linear Regression\n",
    "\n",
    "If we have multiple predictors $X_1, X_2, \\dots, X_p$, the model becomes:\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip} + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j X_{ij} + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Matrix Form\n",
    "\n",
    "Linear regression can be written in matrix notation as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} =\n",
    "\\begin{pmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    "1 & X_{11} & \\dots & X_{1p} \\\\\n",
    "1 & X_{21} & \\dots & X_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & X_{n1} & \\dots & X_{np}\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{\\beta} =\n",
    "\\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_p\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Estimation via Ordinary Least Squares (OLS)\n",
    "\n",
    "The parameters $\\boldsymbol{\\beta}$ are estimated by minimizing the sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} \\left( Y_i - \\hat{Y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "\\hat{Y}_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j X_{ij}\n",
    "$$\n",
    "\n",
    "We minimize:\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} \n",
    "\\left( Y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j X_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "The closed-form OLS solution is:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = \n",
    "(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Residuals\n",
    "\n",
    "The residual for observation $i$ is:\n",
    "\n",
    "$$\n",
    "e_i = Y_i - \\hat{Y}_i\n",
    "$$\n",
    "\n",
    "In vector form:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Core Assumptions\n",
    "\n",
    "For classical linear regression inference to hold:\n",
    "\n",
    "1. **Linearity**\n",
    "   $$\n",
    "   \\mathbb{E}[Y_i \\mid X_i] = \\beta_0 + \\sum_{j=1}^{p} \\beta_j X_{ij}\n",
    "   $$\n",
    "\n",
    "2. **Independence**\n",
    "   $$\n",
    "   \\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0 \\quad \\text{for } i \\neq j\n",
    "   $$\n",
    "\n",
    "3. **Homoscedasticity**\n",
    "   $$\n",
    "   \\text{Var}(\\varepsilon_i \\mid X_i) = \\sigma^2\n",
    "   $$\n",
    "\n",
    "4. **Normality (for inference)**\n",
    "   $$\n",
    "   \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "   $$\n",
    "\n",
    "5. **No Perfect Multicollinearity**\n",
    "   $$\n",
    "   \\mathbf{X}^\\top \\mathbf{X} \\text{ is invertible}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "- $\\beta_0$ = expected value of $Y$ when all predictors equal zero  \n",
    "- $\\beta_j$ = expected change in $Y$ for a one-unit increase in $X_j$, holding other predictors constant  \n",
    "- $\\sigma^2$ = variance of the error term  \n",
    "\n",
    "---\n",
    "\n",
    "# Geometric Interpretation\n",
    "\n",
    "The OLS estimator projects $\\mathbf{y}$ onto the column space of $\\mathbf{X}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n",
    "$$\n",
    "\n",
    "This is the orthogonal projection of $\\mathbf{y}$ onto the linear subspace spanned by the predictors.\n",
    "\n",
    "---\n",
    "\n",
    "# Key Takeaway\n",
    "\n",
    "Linear regression is the foundation of statistical learning:\n",
    "\n",
    "- It formalizes prediction as a projection problem  \n",
    "- It connects geometry, probability, and optimization  \n",
    "- It underlies many machine learning models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32453174-7e6d-42d1-8352-7d77a935afb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Statistical Inference in Linear Regression\n",
    "\n",
    "After estimating the model\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "\n",
    "we often want to test hypotheses about the parameters.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. $t$-Tests for Individual Coefficients\n",
    "\n",
    "To test whether a single coefficient $\\beta_j$ differs from zero:\n",
    "\n",
    "### Null and Alternative Hypotheses\n",
    "\n",
    "$$\n",
    "H_0: \\beta_j = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_1: \\beta_j \\neq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Test Statistic\n",
    "\n",
    "The $t$-statistic is defined as:\n",
    "\n",
    "$$\n",
    "t_j = \\frac{\\hat{\\beta}_j}{\\operatorname{SE}(\\hat{\\beta}_j)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\operatorname{SE}(\\hat{\\beta}_j)\n",
    "=\n",
    "\\sqrt{\\hat{\\sigma}^2 \\left[(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\right]_{jj}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 =\n",
    "\\frac{1}{n - p - 1}\n",
    "\\sum_{i=1}^{n} e_i^2\n",
    "$$\n",
    "\n",
    "Under the null hypothesis:\n",
    "\n",
    "$$\n",
    "t_j \\sim t_{n - p - 1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 2. $F$-Test for Overall Model Significance\n",
    "\n",
    "We test whether *all* predictors jointly have no effect:\n",
    "\n",
    "$$\n",
    "H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_1: \\text{At least one } \\beta_j \\neq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## $F$-Statistic\n",
    "\n",
    "The $F$-statistic compares explained variance to unexplained variance:\n",
    "\n",
    "$$\n",
    "F =\n",
    "\\frac{\n",
    "\\left( \\text{RSS}_0 - \\text{RSS}_1 \\right) / p\n",
    "}{\n",
    "\\text{RSS}_1 / (n - p - 1)\n",
    "}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\text{RSS}_0$ = residual sum of squares of the null model  \n",
    "- $\\text{RSS}_1$ = residual sum of squares of the full model  \n",
    "\n",
    "Under $H_0$:\n",
    "\n",
    "$$\n",
    "F \\sim F_{p,\\, n - p - 1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Nested Model Comparison\n",
    "\n",
    "More generally, to compare:\n",
    "\n",
    "- Reduced model: $\\mathcal{M}_R$\n",
    "- Full model: $\\mathcal{M}_F$\n",
    "\n",
    "The statistic is:\n",
    "\n",
    "$$\n",
    "F =\n",
    "\\frac{\n",
    "(\\text{RSS}_R - \\text{RSS}_F)/(df_R - df_F)\n",
    "}{\n",
    "\\text{RSS}_F/df_F\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Interaction Terms\n",
    "\n",
    "An interaction term allows the effect of one variable to depend on another.\n",
    "\n",
    "For two predictors $X_1$ and $X_2$:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 X_2) + \\varepsilon\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "The marginal effect of $X_1$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Y}{\\partial X_1}\n",
    "=\n",
    "\\beta_1 + \\beta_3 X_2\n",
    "$$\n",
    "\n",
    "The effect of $X_1$ now depends on $X_2$.\n",
    "\n",
    "---\n",
    "\n",
    "## Principle of Marginality\n",
    "\n",
    "If an interaction term is included:\n",
    "\n",
    "$$\n",
    "X_1 X_2\n",
    "$$\n",
    "\n",
    "then the main effects $X_1$ and $X_2$ must also remain in the model.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Bias–Variance Decomposition\n",
    "\n",
    "Suppose the true data-generating process is:\n",
    "\n",
    "$$\n",
    "Y = f(X) + \\varepsilon\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "For a prediction at point $x_0$:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x_0)\n",
    "$$\n",
    "\n",
    "The expected prediction error can be decomposed as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ (Y - \\hat{f}(x_0))^2 \\right]\n",
    "=\n",
    "\\underbrace{\\left( \\operatorname{Bias}[\\hat{f}(x_0)] \\right)^2}_{\\text{Bias}^2}\n",
    "+\n",
    "\\underbrace{\\operatorname{Var}[\\hat{f}(x_0)]}_{\\text{Variance}}\n",
    "+\n",
    "\\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Bias\n",
    "\n",
    "$$\n",
    "\\operatorname{Bias}[\\hat{f}(x_0)]\n",
    "=\n",
    "\\mathbb{E}[\\hat{f}(x_0)] - f(x_0)\n",
    "$$\n",
    "\n",
    "Measures systematic error.\n",
    "\n",
    "---\n",
    "\n",
    "## Variance\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[\\hat{f}(x_0)]\n",
    "=\n",
    "\\mathbb{E} \\left[\n",
    "\\left( \\hat{f}(x_0) - \\mathbb{E}[\\hat{f}(x_0)] \\right)^2\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Measures sensitivity to training data.\n",
    "\n",
    "---\n",
    "\n",
    "# Trade-Off\n",
    "\n",
    "- Simple models → high bias, low variance  \n",
    "- Complex models → low bias, high variance  \n",
    "\n",
    "Optimal prediction balances both.\n",
    "\n",
    "---\n",
    "\n",
    "# Big Picture\n",
    "\n",
    "Linear regression connects:\n",
    "\n",
    "- Optimization  \n",
    "- Probability  \n",
    "- Geometry  \n",
    "- Statistical inference  \n",
    "- Machine learning theory  \n",
    "\n",
    "It is the foundation for:\n",
    "\n",
    "- Logistic regression  \n",
    "- Ridge / Lasso  \n",
    "- Generalized linear models  \n",
    "- Neural networks (linear layers)  \n",
    "- Deep learning  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
